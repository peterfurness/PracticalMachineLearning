---
title: "Machine Learning Assignment"
subtitle: "Qualitative Activity Prediction"
author: "Peter Furness"
date: "October 26, 2016"
output: html_document
---

## Executive Summary
The objective of this assignment was to predict how well a weighlifting exercise was being performed based on data from on-body sensing equipment for a group of six individuals.  The quality of the exercise (the 'how well') was assessed by a five-fold classification (A-E) with category A being correct execution and categories B-E being various types of incorrect execution.

The dataset used to build the predictive model consisted of around twenty thousand session measurements with each observation consisting of on-body sensor data for the session (accelerometer and other measuremements summarised over a 2.5 second interval); as well as the quality classification variable ("classe"), time stamp and user-name variables.

We used machine learning methods to predict the classe variable from the sensor data, based on training and test subsets of the available data.  Three different modelling algorithms were tried:  CART, Linear Discriminant Analysis and Random Forests, all implemented using the caret package in R. Random Forests was found to be the most accurate.  A separate validation subset was used to assess the out of sample accuracy of the RF model; which was found to be around 99%. 

Finally, a separate 'testing' sample of twenty different test cases was processed by the RF model and the results used for the Course Project Prediction Quiz (see Appendix 1).

We wish to thank the authors of the following paper for access to the data used in the assignment:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. http://groupware.les.inf.puc-rio.br/public/papers/2012.Ugulino.WearableComputing.HAR.Classifier.RIBBON.pdf

For more information see: http://groupware.les.inf.puc-rio.br/har (the section on Weight Lifting Exercise WLE dataset)

## Data Load
The following R code loads the two main files - 'training' and 'testing', counts the number of observations (19622 in 'training' and 20 in 'testing') and variables (160) in each.  The 'training' set was further split randomly into 'train' (9619 obs), 'test' (4118 obs) and 'validate' (5885) using the createDataPartition function in caret. The predictor variables (i.e. the data from sensors) were coerced to be numeric as this is the appropriate format for such data.

The 'train' data was used subsequently for all model building, and the 'test' data used to compare the performance of the various models produced.  The 'validate' data was only used at the end of the process to get an estimate of the out of sample accuracy of the final, selected, model.

```{r, cache=TRUE, message = FALSE, warning = FALSE}
## Open required libraries
library(caret); library(randomForest)

set.seed(323233)
## Import training and testing data
fileUrl = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(fileUrl, destfile="pml-training.csv")
training = read.csv("pml-training.csv", stringsAsFactors = FALSE)

fileUrl = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileUrl, destfile="pml-testing.csv")
testing = read.csv("pml-testing.csv", stringsAsFactors = FALSE)

## Convert all potential predictor columns to numeric
nonpredict = c("classe", "X", "user_name", "raw_timestamp_part_1", 
               "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window" )         
## First for training set
z1 = subset(training, select=nonpredict)
z2 = training[ , !(names(training) %in% nonpredict)]
z3 = apply(z2, 2, as.numeric)
training = cbind(z1,z3); rm(z1, z2, z3)

## Next for testing set
nonpredict1 = c( "X", "user_name", "raw_timestamp_part_1", 
               "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window" )         

z1 = subset(testing, select=nonpredict1)
z2 = testing[ , !(names(testing) %in% nonpredict1)]
z3 = apply(z2, 2, as.numeric)
testing = cbind(z1,z3); rm(z1, z2, z3)


## Split the training data into train, test and validation
## Note that the validation set will be used once only 
## to evaluate the performance of the final model
tr1Index = createDataPartition(training$classe, p=0.7,list=FALSE)
tr1 = training[tr1Index,]
validate = training[-tr1Index,]
tr2Index = createDataPartition(tr1$classe,p=0.7,list=FALSE)
train = tr1[tr2Index,]
test = tr1[-tr2Index,]
nrow(train); nrow(test); nrow(validate)
ncol(train); ncol(test); ncol(validate)

```
## Exploratory Analysis
It was clear from eyeballing the data that some of the predictor variables had large proportions of missing values.   The following R code counts the proportions of missing values for all variables in each of the datasets and then removes those variables with high proportions of missing values.

It was found that 60 of the variables had no missing values and the remaining 100 had over 98% missing.  We therefore removed all except these 60 variables from subsequent analysis and modelling.  Further investigation of the variables with missing values was outside the scope of this assignment.

```{r, cache=TRUE}
## Missing values
trainmiss = apply(apply(train, 2, is.na), 2, mean); table(round(trainmiss, 3))
testmiss = apply(apply(test, 2, is.na), 2, mean); table(round(testmiss, 3))
testingmiss = apply(apply(testing, 2, is.na), 2, mean); table(round(testingmiss, 3))

## Remove variables with missing values
remove = names(trainmiss)[trainmiss>0]; length(remove)
remove1 = names(testingmiss)[testingmiss>0]; length(remove1)
## Check the same names to remove in training and testing
mean(remove==remove1)

train = train[ , !(names(train) %in% remove)]
test = test[ , !(names(test) %in% remove)]
validate = validate[ , !(names(validate) %in% remove)]

```
To obtain an overview of the potential usefulness of the various predictor variables for predicting the 'classe' variable we conducted an analysis of variance using a bespoke function 'rsq' which computes the RSquare value for each predictor variable as a predictor of 'classe' on the 'train' dataset.  This was used to rank all the variables.  

The R code to do this, together with a chart showing how the RSquare varies down this ranking is shown below.  Also shown is a list of the top and bottom five ranked predictors as well as boxplots for the top and bottom variables to see how different the discrimination is for the 'classe' variable.

```{r, cache=TRUE, message = FALSE, warning = FALSE}
predictors = names(train)[!(names(train) %in% nonpredict)]
rsq = function(x) {
  u1 = aggregate(x~categ, FUN=mean) 
  u1a = mean(x); v1a = var(x)
  u2 = table(categ)
  u3 = sum(u2)
  u4 = cbind(u1, u1a, u2, u3)
  u5 = sum( (u4[,2]-u1a)^2*u4$Freq)
  u5/v1a/u3
}

t1 = train[, !(names(train) %in% nonpredict)]
categ = train$classe
t2 = apply(t1, 2, rsq)
t3 = as.data.frame(t2)
t4 = cbind(rownames(t3), t3)
t5 = t4[order(t4$t2, decreasing = TRUE),]
colnames(t5)[2] = "rsquare"; colnames(t5)[1] = "predictor"
rownames(t5) = NULL
plot(t5$rsquare, main = "ranked predictor rsquares", 
     xlab = "rank (high to low)", ylab = "rsquare")
points(t5$rsquare, col="red", cex=.5); lines(t5$rsquare)

## Top predictors
head(t5, 5)
## Bottom predictors
tail(t5, 5)
topvar = as.character(t5$predictor)[1]
botvar = as.character(t5$predictor)[length(t5$predictor)]
par(mfrow=c(1,2))
boxplot(pitch_forearm~classe, data = train, col="salmon", 
        main = "top predictor v classe", xlab = "classe", ylab=topvar)
boxplot(gyros_forearm_y~classe, data = train, col="lightblue",
        main = "bottom predictor v classe", xlab = "classe", ylab=botvar)

```

The ranking of predictors in this way might prove useful in determining if there are problematic predictors; for example, predictors which have, in some way, contaminated the outcome variable 'classe' and so appear falaciously highly predictive.  We did not think that any predictors were problematic in this way.

The ranking might also be useful for selecting predictors.  However, as we shall see, models built with all predictors included, proved highly effective and so it was not felt necessary to make any ad-hoc selections in this way.

Similarly, no preprocessing of the data was carried out other than converting predictors to numeric and removing variables with missing values.  If the subsequent modelling had proved problematic then we might have revisited the issue of preprocessing.

## Predictive Modelling
Three different modelling algorithms were tried:  CART, Linear Discriminant Analysis and Random Forests, all implemented using the caret package in R.  The train function in caret was used for the model build in each case, using the 'train' dataset and default settings with no preprocessing. Each algorithm employs 25 bootstrap resamples of the training data. The only cross validation used was an evaluation of the accuracy of each model against the 'test' dataset.
.
```{r, cache=TRUE, message = FALSE, warning = FALSE}
library(caret); library(randomForest)
set.seed(323233)
predictors = names(train)[!(names(train) %in% nonpredict)]

## Classification tree model build
fit_rpart = train(classe~., data=train[, c("classe", predictors)], method="rpart")
fit_rpart
confusionMatrix(predict(fit_rpart, newdata=test), test$classe)

## Linear discriminant model build
fit_lda = train(classe~., data=train[, c("classe", predictors)], method="lda")
fit_lda
confusionMatrix(predict(fit_lda, newdata=test), test$classe)
```
```{r, cache=TRUE, message = FALSE, warning = FALSE}
## Random forest model build
fit_rf = train(classe~., data=train[, c("classe", predictors)], method="rf")
fit_rf
confusionMatrix(predict(fit_rf, newdata=test), test$classe)

```
The results from the Random Forest model were so impressive on the 'test' dataset (>98% accuracy) that it was not felt necessary to do any further tinkering with the selection and preprocesing of predictors or with the choice of modelling algorithm.

We should point out that the RF model took significantly longer to train than the other methods (around 40 minutes compared with a few seconds on the machine used by the author); though the RF model did not appear particularly slow when it came to the prediction tasks.  It might be possible to improve the accuracy of the LDA method by using ingenious preprocessing; in which case training time would be less of an issue. However we did not feel this was necessary for this assignment.

## Out of Sample Accuracy
The following R code evaluates the RF model on the 'validate' dataset.  This shows >98% accuracy. However, we should point out that all the modelling and testing was done using a sample of measurements on just six individuals.  If the model were to be applied to data for a different collection of individuals, or for the same individuals in very different time frames, we would not expect to see such impressive results.

```{r, cache=TRUE, message = FALSE, warning = FALSE}
## Random Forest results against validation set
confusionMatrix(predict(fit_rf, newdata=validate), validate$classe)
```

## Conclusion
This work shows that is is possible to predict with a high degree of accuracy whether or not an individual is correctly executing a weightlifting exercise.  Moreover we have shown, at least for this particular data, that Random Forests outperform CART and Linear Disciminant Analysis.  

We should point out that this very high level of accuracy might not be maintained if the trained RF model were applied to data for different individuals or at different times.



# Appendix 

## Testing Data Results
```{r, cache=TRUE}
cbind(predict(fit_rf, newdata=testing), testing[,1:3])

```
